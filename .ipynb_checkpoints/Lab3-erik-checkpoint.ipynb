{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Exploring CFIR Image Data\n",
    "\n",
    "Erik Gabrielsen, Danh Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding \n",
    "\n",
    "Human beings are very adept at recognizing objects and patterns in images, but the same cannot be said about computers. As we push forward in the field of computer vision, the [CFIR-10](https://www.kaggle.com/c/cifar-10) dataset was used in a Kaggle competition to programatically recognize objects in images. The dataset consists of 50,000 training images with the following labels: \n",
    "\n",
    "* airplane \n",
    "* automobile \n",
    "* bird \n",
    "* cat \n",
    "* deer \n",
    "* dog \n",
    "* frog \n",
    "* horse \n",
    "* ship \n",
    "* truck\n",
    "\n",
    "The images are 32x32, colored with the subject mostly centered in the middle of the image. Their subjects are various types, shapes, and orientations. For instance, birds and cats can be of different species and sizes, and images of truck can be taken head on, from behind, or from the side. The dataset also contains 10,000 test images to classify for the competition, as well as 290,000 \"junk\" images that are not scored.  \n",
    "\n",
    "In the first lab, we will explore certain component analysis and feature extraction to see if there are common differences among the classes. The ultimate goal is to programmatically determine what type of object appears in an image. We hope to achieve above 50% accuracy on our training and testing sets one day. In the interest of runtime, we will look at the 5,000 images of the training set. In these first images, the distribution of images per label is pretty even at around 500 images each.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\PrestonSSD2/Downloads/train/1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-81d1120e9059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\PrestonSSD2\\Anaconda3\\envs\\mlip\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\PrestonSSD2/Downloads/train/1.png'"
     ]
    }
   ],
   "source": [
    "directory = os.path.expanduser('~/Downloads/train/')\n",
    "images = []\n",
    "files = []\n",
    "\n",
    "# loop over directories: \n",
    "for i in range(1, 50001):\n",
    "    name = str(i) + '.png'\n",
    "    file = os.path.join(directory, name)\n",
    "    images.append(mpimg.imread(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in label\n",
    "df = pd.read_csv('~/Downloads/trainLabels.csv') # read in the csv file\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "The images are initially stored as 3 separate matrices for Red, Green, and Blue values. We converted these 3 matrices into 1 grayscale image according to the luminance formula: `0.299*R + 0.587*G, 0.114*B`. Examples of the grayscale images are displayed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to luminance, thanks http://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "for i in range(0, len(images)):\n",
    "    images[i] = rgb2gray(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a helper plotting function\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_gallery(images, labels, 32, 32) # defaults to showing a 3 by 6 subset of the faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linearization\n",
    "img_concat = []\n",
    "for image in images: \n",
    "    img_concat.append(np.concatenate(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img_concat_arr = np.array(img_concat)\n",
    "print(len(img_concat))\n",
    "print(np.array(img_concat[0:50000]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate each image matrix to represent it as one linear array of image features. As previously stated, we are looking at only 5,000 images in order to save time running analysis and feature extraction algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get some of the specifics of the dataset\n",
    "# y = lfw_people.target\n",
    "X = np.array(img_concat[0:5000]) \n",
    "names = labels[0:5000]\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "h, w = 32, 32\n",
    "n_classes = len(names)\n",
    "\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))\n",
    "print(\"n_classes: {}\".format(n_classes))\n",
    "print(\"Original Image Sizes {}by{}\".format(h,w))\n",
    "print (125*94) # the size of the images are the size of the feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full PCA\n",
    "\n",
    "We first performed a linear dimensionality reduction on the images, using full principal component analysis. We plot the number of principal components versus the cumulative explained variance by those components, and found that in order to explain about 90% of the variance, we need around 73 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenobjects from %d objects\" % (\n",
    "    n_components, X.shape[0]))\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "%time pca.fit(X)\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Full PCA](https://raw.githubusercontent.com/egabrielsen/MachineLearning/master/Lab03/Screen%20Shot%202017-02-17%20at%208.14.17%20PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot tell much from the eigenvalues extracted from each image. However, a reconstruction of an automobile shows a very close representation of the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigen_titles = [\"eigenobject %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigen_titles, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reconstruct_image(trans_obj,org_features):\n",
    "    low_rep = trans_obj.transform(org_features)\n",
    "    rec_image = trans_obj.inverse_transform(low_rep)\n",
    "    return low_rep, rec_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_reconstruct = 4    \n",
    "low_dimensional_representation, reconstructed_image = reconstruct_image(pca,X[idx_to_reconstruct])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[4].reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Reconstructed from Full PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized PCA\n",
    "\n",
    "We then performed another linear dimensionality reduction, this time using randomized principal component analysis. When plotting the number of components by the explained variance, we found that in order to explain 90% of the variance, we also need around 73 components. The eigenvalues of the objects and reconstruction of an automobile is similar to the results obtained from full PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenvalues from %d objects\" % (\n",
    "    n_components, X.shape[0]))\n",
    "\n",
    "rpca = RandomizedPCA(n_components=n_components)\n",
    "%time rpca.fit(X)\n",
    "eigenfaces = rpca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Randomized PCA](https://raw.githubusercontent.com/egabrielsen/MachineLearning/master/Lab03/Screen%20Shot%202017-02-17%20at%208.14.39%20PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_explained_variance(rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigenface_titles = [\"eigenvalue %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.io import imshow\n",
    "# idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "idx_to_reconstruct = 4\n",
    "low_dimensional_representation, reconstructed_image = reconstruct_image(rpca,X[idx_to_reconstruct])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[idx_to_reconstruct].reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Reconstructed from Random PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA- Nonlinear Reduction\n",
    "\n",
    "We used kernel principal component analysis to perform a non-linear dimensionality reduction. Upon quick inspection, the automobile reconstructed from kernel PCA appear to match the original picture more closely than the linear reduction methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "to \n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenvalues from %d objects\" % (n_components, X.shape[0]))\n",
    "\n",
    "kpca = KernelPCA(n_components=n_components, kernel='rbf', \n",
    "                fit_inverse_transform=True, gamma=15) # very sensitive to the gamma parameter\n",
    "%time kpca.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_to_reconstruct = 4    \n",
    "low_dimensional_representation, reconstructed_image = reconstruct_image(kpca,X[idx_to_reconstruct])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[4].reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Original')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.title('Reconstructed from Kernel PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear vs. Non-linear PCA\n",
    "\n",
    "After comparing the reconstructions from all three PCA's below, the Kernel PCA reconstruction can most accurately reflect the original picture. However, Randomized PCA is not far behind. There are cases where the Kernel PCA reconstruction is not very similar to the original. When that occurs, Kernal PCA appears wrong by a huge margin. Randomized PCA is more frequently similar to the original image. For this reason, we would prefer to use randomized PCA. We would rather be a tiny bit less accurate all the time, than dead wrong several times when trying to reconstruct an image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "from ipywidgets import widgets  # make this interactive!\n",
    "# compare the different methods\n",
    "\n",
    "def plt_reconstruct(idx_to_reconstruct):\n",
    "    idx_to_reconstruct = np.round(idx_to_reconstruct)\n",
    "    \n",
    "    reconstructed_image = pca.inverse_transform(pca.transform(X[idx_to_reconstruct]))\n",
    "    reconstructed_image_rpca = rpca.inverse_transform(rpca.transform(X[idx_to_reconstruct]))\n",
    "    reconstructed_image_kpca = kpca.inverse_transform(kpca.transform(X[idx_to_reconstruct]))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(X[idx_to_reconstruct].reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Original')\n",
    "    \n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(reconstructed_image.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Full PCA')\n",
    "    \n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(reconstructed_image_rpca.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Randomized PCA')\n",
    "    \n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(reconstructed_image_kpca.reshape((h, w)), cmap=plt.cm.gray)\n",
    "    plt.title('Kernel PCA')\n",
    "    \n",
    "widgets.interact(plt_reconstruct,idx_to_reconstruct=(0,n_samples-1,1),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daisy Feature Extraction \n",
    "\n",
    "In order to test out feature extraction, we first applied the DAISY descriptor to the images. The first image below illustrates how the descriptor will be applied onto our 32 x 32 image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "# lets first visualize what the daisy descriptor looks like\n",
    "features, img_desc = daisy(img,step=40, radius=10, rings=3, histograms=5, orientations=8, visualize=True)\n",
    "imshow(img_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = daisy(img,step=10, radius=10, rings=2, histograms=4, orientations=8, visualize=False)\n",
    "print(features.shape)\n",
    "print(features.shape[0]*features.shape[1]*features.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a function to tak in the row of the matric and return a new feature\n",
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape),step=10, radius=10, rings=2, histograms=6, orientations=8, visualize=False)\n",
    "    return feat.reshape((-1))\n",
    "\n",
    "%time test_feature = apply_daisy(X[3],(h,w))\n",
    "test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply to entire data, row by row,\n",
    "# takes about a minute to run\n",
    "%time daisy_features = np.apply_along_axis(apply_daisy, 1, X, (h,w))\n",
    "print(daisy_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix = pairwise_distances(daisy_features,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the DAISY descriptor to all images in the dataset, we wanted a quick visualization of whether or not DAISY would find objects of the same class. The filter does indeed closely pair two images of an automobile. While the second image is less clear, the pair does seem to have similar size and angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# find closest image to current image\n",
    "idx1 = 5\n",
    "distances = copy.deepcopy(dist_matrix[idx1,:])\n",
    "distances[idx1] = np.infty # dont pick the same image!\n",
    "idx2 = np.argmin(distances)\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.subplot(1,2,1)\n",
    "imshow(X[idx1].reshape((h,w)))\n",
    "plt.title(\"Original Image: \"+names[idx1])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "imshow(X[idx2].reshape((h,w)))\n",
    "plt.title(\"Closest Image: \"+names[idx1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When extending our inspection to three images, DAISY operator alone does not do as well. For the image below (image 827) matches two birds of similar shapes and angle, but its third closest match appears to be a fighter jet, though one that is also a similar shape to the two birds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import fixed\n",
    "from ipywidgets import widgets\n",
    "# put it together inside a nice widget\n",
    "def closest_image(dmat,idx1):\n",
    "    distances = copy.deepcopy(dmat[idx1,:]) # get all image diatances\n",
    "    distances[idx1] = np.infty # dont pick the same image!\n",
    "    idx2 = np.argmin(distances)\n",
    "    \n",
    "    distances[idx2] = np.infty\n",
    "    idx3 = np.argmin(distances)\n",
    "    \n",
    "    plt.figure(figsize=(10,16))\n",
    "    plt.subplot(1,3,1)\n",
    "    imshow(X[idx1].reshape((h,w)))\n",
    "    plt.title(\"Original Image: \"+names[idx1])\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    imshow(X[idx2].reshape((h,w)))\n",
    "    plt.title(\"Closest Image:  \"+names[idx2])\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    imshow(X[idx3].reshape((h,w)))\n",
    "    plt.title(\"Next Closest Image: \"+names[idx3])\n",
    "    \n",
    "widgets.interact(closest_image,idx1=(0,n_samples-1,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor Feature Extraction\n",
    "\n",
    "We perform another feature extraction using Gabor filters, which are used to detect textures and edges. By getting the Gabor feature set for each image in the data set, we can compare the distance matrices to one another in order to fine the next closest image. This can be useful to see if using Gabor features will be a good way to recognize objects of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import stats\n",
    "\n",
    "# prepare filter bank kernels\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta,\n",
    "                                          sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n",
    "\n",
    "            \n",
    "# compute the filter bank and take statistics of image\n",
    "def compute_gabor(row, kernels, shape):\n",
    "    feats = np.zeros((len(kernels), 4), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(row.reshape(shape), kernel, mode='wrap')\n",
    "        _,_,feats[k,0],feats[k,1],feats[k,2],feats[k,3] = stats.describe(filtered.reshape(-1))\n",
    "        # mean, var, skew, kurt\n",
    "        \n",
    "    return feats.reshape(-1)\n",
    "\n",
    "idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "\n",
    "gabr_feature = compute_gabor(X[idx_to_reconstruct], kernels, (h,w))\n",
    "gabr_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes ~3 minutes to run entire dataset\n",
    "%time gabor_stats = np.apply_along_axis(compute_gabor, 1, X, kernels, (h,w))\n",
    "print(gabor_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix = pairwise_distances(gabor_stats,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gabor features, we tested out the closest match of an automobile, and it does indeed return an image of an automobile. In fact, we could not even tell the second black and white image was an automobile without the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find closest image to current image\n",
    "idx1 = 5\n",
    "distances = copy.deepcopy(dist_matrix[idx1,:])\n",
    "distances[idx1] = np.infty # dont pick the same image!\n",
    "idx2 = np.argmin(distances)\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.subplot(1,2,1)\n",
    "imshow(X[idx1].reshape((h,w)))\n",
    "plt.title(\"Original Image: \"+names[idx1])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "imshow(X[idx2].reshape((h,w)))\n",
    "plt.title(\"Closest Image: \"+names[idx1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below (image 827), we can see that the next closest image is indeed a bird (the same class as the original), but the next is an airplane. Tiny photos of birds and airplanes may be very similar because of their shapes, edges, and the fact that they are likely to have similar white space in the background. Comparing the same image search using the DAISY descriptor, the \"bird\" matches here have similar shape but different orientations. For many different original images, we are able to get the closest, and next closest images to match, however we are no where near above 50% in computing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "widgets.interact(closest_image,idx1=(0,n_samples-1,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Gabor Features\n",
    "\n",
    "We decided to proceed our analysis of Gabor features, as we attempt to see if there are distinct differences among the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = np.arange(0, 66, 1)\n",
    "col_vector = np.array(names)\n",
    "# create column vector and add it to matrix with label\n",
    "cols_test = np.column_stack((col_vector,col_vector))\n",
    "gabor = np.hstack((gabor_stats, cols_test))\n",
    "df = pd.DataFrame(gabor, columns=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below is a comparison of the top 5 gabor features for all 10 classes within this data set. It is hard to distinguish the difference between all of them. There is a slight different in spread among some classes, but most have similar distributions. Gabor feature extraction alone may not be the best way to filter the images in this data set. Further below, we will examine the plots with only 2 classes at a time to see if there is a more noticable difference among the plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "deleted_cols = np.arange(5,65,1)\n",
    "\n",
    "#only use the top 5 gabor values in feature pair plots\n",
    "df.drop(df.columns[deleted_cols], axis=1, inplace=True)\n",
    "df.head()\n",
    "sns.set()\n",
    "sns.pairplot(df,hue=65, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the heat map of the gabor feature correlations. This is only depicting the top 5 Gabor features but if we were to expand this, the heat map starts to repeat and become a checkered pattern. Since Gabor filters are applied at different orientations, this create a repeating pattern of highly correlated, and conversely, negatively correlated features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = np.arange(0,64,1)\n",
    "df_no_labels = pd.DataFrame(gabor_stats, columns=cols)\n",
    "deleted_cols = np.arange(5,64,1)\n",
    "\n",
    "#only use the top 5 gabor values in feature pair plots\n",
    "df_no_labels.drop(df_no_labels.columns[deleted_cols], axis=1, inplace=True)\n",
    "sns.heatmap(df_no_labels.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to examine the classes two at a time. The plot below is a comparison of the top 5 gabor features for a truck and a bird determined during feature extraction. We can see that the distribution of features for trucks are closer together, whereas the one for birds  are more varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_truck = df_copy[df_copy[65] == 'truck']\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_frog = df_copy[df_copy[65] == 'bird']\n",
    "bigdata = df_frog.append(df_truck, ignore_index=True)\n",
    "\n",
    "sns.set()\n",
    "sns.pairplot(bigdata,hue=65, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compared the distributions of Gabor filters between cat images and ship images. There is more overlap between the distributions compared to birds vs trucks. Differentiating between cat pictures and ship pictures may be difficult with just Gabor filters alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_truck = df_copy[df_copy[65] == 'ship']\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_frog = df_copy[df_copy[65] == 'cat']\n",
    "bigdata = df_frog.append(df_truck, ignore_index=True)\n",
    "\n",
    "sns.set()\n",
    "sns.pairplot(bigdata,hue=65, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a biplot of the feature set given by Gabor feature extraction. Unfortunately there isnt much that we can deduct from this graph other than that it is very hard to discern between the Gabor features. Its interesting to see how most of the cumulative variance is left of the y axis. This may be because there are too many data points and that we need to further break down the features in order to get some meaningful relation out of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def biplot(pca, dat, title=''):\n",
    "    \n",
    "    import plotly\n",
    "    from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis, Bar, Line\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "    \n",
    "    # 0,1 denote PC1 and PC2; change values for other PCs\n",
    "    xvector = pca.components_[0] \n",
    "    yvector = pca.components_[1]\n",
    "\n",
    "    tmp = pca.transform(dat.values)\n",
    "    xs = tmp[:,0] \n",
    "    ys = tmp[:,1]\n",
    "\n",
    "    annotations = [Scatter(x=xs, y=ys, mode ='markers', name='cumulative explained variance')]\n",
    "    for i in range(len(xvector)):\n",
    "        txt = list(dat.columns.values)[i]\n",
    "        annotations.append(\n",
    "                Scatter(\n",
    "                    x=[0, xvector[i]*max(xs)],\n",
    "                    y=[0, yvector[i]*max(ys)],\n",
    "                    mode='lines+text',\n",
    "                    text=['', txt],\n",
    "                    name=txt,\n",
    "                ))\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": annotations,\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal Component One'), \n",
    "                         yaxis=YAxis(title='Principal Component Two'),\n",
    "                        title=title)\n",
    "    })\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "X2 = gabor_stats\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X2) \n",
    "columns = np.arange(0,64,1)\n",
    "\n",
    "biplot(pca,pd.DataFrame(gabor_stats,columns=columns),'Gabor Feature Biplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a graph of the explained variance ratio when using the gabor features as the principle components. Here we can see that we would just need 6 gabor features in order to see a variance ratio above 0.96. This is conciderably better than what we had before by just examining the variance pixel by pixel of the image. By using the pixels as the principle components, we would need around 100 pixels/features in order to see a 0.92 ratio or better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gabor](https://raw.githubusercontent.com/egabrielsen/MachineLearning/master/Lab03/Screen%20Shot%202017-02-17%20at%209.10.51%20PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=64)\n",
    "X_pca = pca.fit(gabor_stats)\n",
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
